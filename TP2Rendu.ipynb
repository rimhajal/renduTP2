{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'TP2 : Les Arbres'\n",
        "author: Rim ALHAJAL\n",
        "format: pdf\n",
        "editor: visual\n",
        "---"
      ],
      "id": "08faa46b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Arbres de Décision - Algorithme CART\n",
        "\n",
        "<a name=\"Arbre de Décision - Algorithme CART\"></a>\n",
        "\n",
        "## Introduction\n",
        "\n",
        "<a name=\"Introduction\"></a>\n",
        "\n",
        "Publié par Leo Breiman en 1984, l'algorithme CART (Classification and Regression Trees) servira comme une méthode pour construire des arbres de décision pouvant être utilisés à la fois pour la classification et la régression. Depuis, CART est devenu un algorithme fondamental dans le domaine de l'apprentissage automatique et de l'analyse de données.\n",
        "\n",
        "## Mesure d'Homogénéité\n",
        "\n",
        "<a name=\"Mesure d'Homogénéité\"></a>\n",
        "\n",
        "Dans le cadre de la régression, on peut mesurer l'homogénéité à partir du calcul de la variance. Effectivement, si les données sont plus \"variées\" entre eux, elles deviennent plus hétérogènes. Ainsi, l'homogénéité est donné par la variance minimale.\n",
        "\n",
        "## Simulations\n",
        "\n",
        "<a name=\"Simulations\"></a>\n",
        "\n",
        "Importons les packages nécessaires en premier :\n"
      ],
      "id": "99e18cca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: FALSE\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import rc\n",
        "import graphviz\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "sys.path.append('./Code/')\n",
        "\n",
        "from sklearn import tree, datasets\n",
        "from tp_arbres_source import (rand_gauss, rand_bi_gauss, rand_tri_gauss,\n",
        "                              rand_checkers, rand_clown,\n",
        "                              plot_2d, frontiere)"
      ],
      "id": "884191ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En utilisant la fonction `rand_checkers`, on construit un échantillon de taille $n=456$ bien équilibré.\n"
      ],
      "id": "1b8f28ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "data = rand_checkers(n1=114,n2=114,n3=114,n4=114,sigma=0.1)\n",
        "n_samples = len(data)\n",
        "X_train = data[:, :2]\n",
        "Y_train = data[:, 2].astype(int)"
      ],
      "id": "923635f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ensuite, on calcul le pourcentage d'erreurs comises en fonction de la profondeur maximale des arbres de décisions selon les critères \"indice de Gini\" et \"entropie\". Pour exécuter cette étape, il faut importer le module `tree` de `sklearn`.\n"
      ],
      "id": "2a5247da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "dt_entropy = tree.DecisionTreeClassifier(criterion='entropy')\n",
        "dt_gini = tree.DecisionTreeClassifier(criterion='gini')\n",
        "\n",
        "dmax = 12\n",
        "scores_entropy = np.zeros(dmax)\n",
        "scores_gini = np.zeros(dmax)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(dmax):\n",
        "    dt_entropy = tree.DecisionTreeClassifier(criterion='entropy',max_depth=i+1)\n",
        "    dt_entropy.fit(X_train,Y_train)\n",
        "    scores_entropy[i] = dt_entropy.score(X_train, Y_train)\n",
        "\n",
        "    dt_gini = tree.DecisionTreeClassifier(criterion='gini',max_depth=i+1)\n",
        "    dt_gini.fit(X_train,Y_train)\n",
        "    scores_gini[i] = dt_gini.score(X_train, Y_train)\n",
        "\n",
        "    plt.subplot(3, 4, i + 1)\n",
        "    frontiere(lambda x: dt_gini.predict(x.reshape((1, -1))), X_train, Y_train, step=50, samples=False)\n",
        "plt.draw()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(1-scores_entropy, label='entropy')\n",
        "plt.plot(1-scores_gini, label='gini')\n",
        "plt.xlabel('Max depth')\n",
        "plt.ylabel('Accuracy Score')\n",
        "plt.legend()\n",
        "plt.title(\"Comparing Entropy and Gini Criterion Accuracy Scores in terms of Max Depth\")\n",
        "plt.draw()\n",
        "print(\"Scores with entropy criterion: \", scores_entropy)\n",
        "print(\"Scores with Gini criterion: \", scores_gini)"
      ],
      "id": "ecd96bc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On remarque que l'erreur diminue pour les deux courbes semblablement et presque s'annule à partir d'une profondeur maximale égale à 10. L'idée est de trouver une profondeur maximale idéale qui servira à éviter le sur-apprentissage et le sous-apprentissage.\n",
        "\n",
        "## Classification pour une Erreur Minimale (Entropie)\n",
        "\n",
        "<a name=\"Classification pour une Erreur Minimale (Entropie)\"></a>\n",
        "\n",
        "Avec la fonction `frontiere`, on obtient la classification obtenue avec l'entropie pour une profondeur maximale choisie arbitrairement. Ici, on prend `max_depth = 10`.\n"
      ],
      "id": "a66994a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "dt_entropy.max_depth = 10\n",
        "\n",
        "plt.figure()\n",
        "frontiere(lambda x: dt_entropy.predict(x.reshape((1, -1))), X_train, Y_train, step=100)\n",
        "plt.title(\"Frontier with Minimum Error from Entropy Criterion\")\n",
        "plt.draw()\n",
        "print(\"Best score with entropy criterion: \", dt_entropy.score(X_train, Y_train))"
      ],
      "id": "13bfd4b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arbres de Décision\n",
        "\n",
        "<a name=\"Arbres de Décisions\"></a>\n",
        "\n",
        "Avec le package `graphviz`, on est capable de visualiser l'arbre de décision des données. On obtient le résultat suivant :\n",
        "\n",
        "<img src=\"Schémas/Arbre_Q4.png\" width=\"800\"/>\n",
        "\n",
        "Un arbre de décision s'agit d'un nœud principal appelé \"racine\" donnant deux nœuds enfants. Chaque nœud non-terminal engendre à son tour deux autres nœuds, et ainsi de suite. À mesure que l'arbre progresse, nous atteignons finalement les nœuds terminaux, où les décisions sont prises.\n",
        "\n",
        "Si la condition au niveau du nœud $k$ est satisfaite, nous empruntons la branche de gauche ; autrement, nous suivons la branche de droite.\n",
        "\n",
        "## Nouveau Echantillon\n",
        "\n",
        "<a name=\"Nouveau Echantillon\"></a>\n",
        "\n",
        "Reprenons la même démarche qu'avant mais avec de nouvelles données.\n"
      ],
      "id": "06721add"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "data_test = rand_checkers(40, 40, 40, 40)\n",
        "X_test = data_test[:, :2]\n",
        "Y_test = data_test[:, 2].astype(int)\n",
        "\n",
        "dmax = 20\n",
        "scores_entropy = np.zeros(dmax)\n",
        "scores_gini = np.zeros(dmax)\n",
        "plt.figure(figsize=(6, 7))\n",
        "\n",
        "for i in range(dmax):\n",
        "    dt_entropy = tree.DecisionTreeClassifier(criterion='entropy',\n",
        "                                             max_depth=i + 1)\n",
        "    dt_entropy.fit(X_train, Y_train)\n",
        "    scores_entropy[i] = dt_entropy.score(X_test, Y_test)\n",
        "\n",
        "    dt_gini = tree.DecisionTreeClassifier(criterion='gini', max_depth=i+1)\n",
        "    dt_gini.fit(X_train, Y_train)\n",
        "    scores_gini[i] = dt_gini.score(X_test, Y_test)\n",
        "\n",
        "plt.draw()\n",
        "plt.figure()\n",
        "plt.plot(1-scores_entropy, label='entropy')\n",
        "plt.plot(1-scores_gini, label='gini')\n",
        "plt.legend()\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Testing Error')\n",
        "print(1-scores_entropy)"
      ],
      "id": "ef509723",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comme dans la première partie, les erreurs des deux critères se comportent de la même manière. On note qu'avec une taille de n = 160 \\< 456, l'erreur s'annule plus rapidement. Dès une profondeur maximale de 9, il est clair qu'il sera inutile d'augmenter la profondeur vue que l'erreur est pratiquement stable à ce point là.\n",
        "\n",
        "## Dataset `DIGITS`\n",
        "\n",
        "<a name=\"Dataset `DIGITS`\"></a>\n",
        "\n",
        "On effectue encore une fois la même étude mais cette fois-ci sur les données `DIGITS` disponibles sur Python dans le module `sklearn.datasets`. On découpe tout d'abord les données en \"testing set\" et \"training set\" et ceci à travers la fonction `train_test_split`.\n"
      ],
      "id": "8fd6bf5b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "ntest = len(digits.data)-int(len(digits.data)*0.8)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "X_train = digits.data[:ntest]\n",
        "Y_train = digits.target[:ntest]\n",
        "X_test = digits.data[ntest:]\n",
        "Y_test = digits.target[ntest:]\n",
        "\n",
        "dt_entropy = tree.DecisionTreeClassifier(criterion='entropy')\n",
        "dt_gini = tree.DecisionTreeClassifier(criterion='gini')\n",
        "\n",
        "dt_entropy.fit(X_train, Y_train)\n",
        "dt_gini.fit(X_train, Y_train)\n",
        "\n",
        "print(dt_entropy)\n",
        "print(dt_entropy.get_params())"
      ],
      "id": "090ae0fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Maintenant, on s'intéresse à l'erreur :\n"
      ],
      "id": "37329572"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "dmax = 15\n",
        "scores_entropy = np.zeros(dmax)\n",
        "scores_gini = np.zeros(dmax)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(dmax):\n",
        "    dt_entropy = tree.DecisionTreeClassifier(criterion='entropy',\n",
        "                                             max_depth=i+1)\n",
        "    dt_entropy.fit(X_train, Y_train)\n",
        "    scores_entropy[i] = dt_entropy.score(X_train, Y_train)\n",
        "\n",
        "    dt_gini = tree.DecisionTreeClassifier(criterion='gini', max_depth=i+1)\n",
        "    dt_gini.fit(X_train, Y_train)\n",
        "    scores_gini[i] = dt_gini.score(X_train, Y_train)\n",
        "plt.draw()\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(1-scores_entropy, label='entropy')\n",
        "plt.plot(1-scores_gini, label='gini')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Training Error')\n",
        "plt.legend([\"entropy\", \"gini\"])\n",
        "plt.draw()\n",
        "print(\"Error with entropy criterion: \", 1-scores_entropy)\n",
        "print(\"Error with Gini criterion: \", 1-scores_gini)"
      ],
      "id": "adab734a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On remarque ici que pour les données d'entrainement, le graphe renseigne une erreur nulle au bout d'une profondeur maximale égale à 8. Ceci est cohérent avec le travail d'avant. Ce qui est plus important, c'est l'erreur avec les données test :\n"
      ],
      "id": "eaa2ad2c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "dmax = 15\n",
        "scores_entropy = np.zeros(dmax)\n",
        "scores_gini = np.zeros(dmax)\n",
        "\n",
        "plt.figure(figsize=(6, 7))\n",
        "for i in range(dmax):\n",
        "    dt_entropy = tree.DecisionTreeClassifier(criterion='entropy',\n",
        "                                             max_depth=i + 1)\n",
        "    dt_entropy.fit(X_train, Y_train)\n",
        "    scores_entropy[i] = dt_entropy.score(X_test, Y_test)\n",
        "\n",
        "    dt_gini = tree.DecisionTreeClassifier(criterion='gini', max_depth=i+1)\n",
        "    dt_gini.fit(X_train, Y_train)\n",
        "    scores_gini[i] = dt_gini.score(X_test, Y_test)\n",
        "\n",
        "plt.draw()\n",
        "plt.figure()\n",
        "plt.plot(1-scores_entropy, label='entropy')\n",
        "plt.plot(1-scores_gini, label='gini')\n",
        "plt.legend([\"entropy\", \"gini\"])\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Accuracy Score')\n",
        "print(scores_entropy)"
      ],
      "id": "319118f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Avec les données test, on trouve une différence avec l'entropie et l'indice de gini. En effet, l'entropie aboutit à de meilleurs résultats et des erreurs plus petites que celles obtenues avec l'indice de gini. De plus, la chute des erreurs est plus rapide avec l'entropie qu'avec l'indice de gini. D'après le graphe, on peut dire que 6 est un bon choix de profondeur.\n",
        "\n",
        "#Méthodes de Choix de Paramètres - Sélection de modèle\n",
        "<a name=\"Méthodes de Choix de Paramètres - Sélection de modèle\"></a>\n",
        "\n",
        "## Validation Croisée\n",
        "<a name=\"Validation Croisée\"></a>\n",
        "\n",
        "La fonction `cross_val_score` effectue une validation croisée afin de déterminer la profondeur optimale de l'arbre qui minimise l'erreur. Elle prend en entrée un arbre, un critère de sélection, une profondeur maximale, ainsi que les données d'observation $X$ et les réponses $y$.\n",
        "\n",
        "Cette fonction divise l'ensemble de données $X$ en un ensemble d'apprentissage et un ensemble de test. L'arbre est entraîné sur l'ensemble d'apprentissage, puis testé sur l'ensemble de test, et enfin, la précision des prédictions est évaluée par rapport aux réponses $y$. Ce processus est répété plusieurs fois, généralement 10 fois (paramètre cv=10`), avec des divisions différentes entre les ensembles d'apprentissage et de test.\n",
        "\n",
        "À la fin de ces répétitions, on obtient un vecteur contenant les scores obtenus pour chaque itération. Ensuite, ces scores sont moyennés. Cette procédure est répétée à plusieurs itérations afin de déterminer la profondeur optimale qui donne l'erreur minimale.\n"
      ],
      "id": "791f5a27"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(256)\n",
        "\n",
        "error_ent = []\n",
        "error_gini = []\n",
        "dmax = 20\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "for i in range(dmax):\n",
        "    dt_entropy = tree.DecisionTreeClassifier(criterion='entropy',\n",
        "                                             max_depth=i + 1)\n",
        "    accuracy = cross_val_score(dt_entropy, X, y)\n",
        "    error_ent.append(1-accuracy.mean())\n",
        "    dt_gini = tree.DecisionTreeClassifier(criterion='gini',\n",
        "                                          max_depth=i + 1)\n",
        "    accuracy2 = cross_val_score(dt_gini, X, y)\n",
        "    error_gini.append(1-accuracy2.mean())\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(error_ent, label=\"entropy\")\n",
        "plt.plot(error_gini, label=\"gini\")\n",
        "plt.xlabel('Depth')\n",
        "plt.ylabel(\"Error\")\n",
        "plt.legend()\n",
        "plt.title(\"Error with entropy and gini criterion\")\n",
        "plt.show()\n",
        "\n",
        "print(error_ent)\n",
        "print(error_gini)\n",
        "best_depth = np.argmin(error_ent) + 1\n",
        "print(best_depth)"
      ],
      "id": "96bd805c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On obtient une profondeur maximale optimale de 15. \n",
        "Il est possible d'obtenir des résultats différents avec la méthode de validation croisée pour une même dataset. Or, grâce à la moyenne des résultats, les valeurs finales sont plus ou moins régulées.\n",
        "\n",
        "## Courbe d'Apprentissage\n",
        "<a name=\"Courbe d'Apprentissage\"></a>\n",
        "\n",
        "Une courbe d'apprentissage est une représentation graphique montrant l'évolution du score d'un modèle d'apprentissage automatique avec l'augmentation de données d'entraînement. \n"
      ],
      "id": "668b765a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = digits.data\n",
        "y = digits.target\n",
        "model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=best_depth)\n",
        "\n",
        "n_samples, train_scores, test_scores = learning_curve(model, X, y, cv=5)\n",
        "\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.xlabel(\"Training Set Size\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid()\n",
        "plt.fill_between(n_samples, train_scores_mean - train_scores_std,\n",
        "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                 color=\"blue\")\n",
        "plt.fill_between(n_samples, test_scores_mean - test_scores_std,\n",
        "                 test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                 color=\"red\")\n",
        "plt.plot(n_samples, train_scores_mean, 'o-', color=\"blue\",\n",
        "         label=\"Training Score\")\n",
        "plt.plot(n_samples, test_scores_mean, 'o-', color=\"red\",\n",
        "         label=\"Cross Validation Score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "b5399556",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La courbe en bleue correspond aux scores des données d'entrainement. Le score est constant pour une valeur de 1, c'est-à-dire le modèle a bien appris sur les données.\n",
        "La courbe en rouge correspond aux scores de validation croisée augmente avec la taille du training set, même arrivant à un score de 0.8. Ceci signifie que le modèle ne se trouve ni dans une situation de sur-ajustement ni de sous-ajustement. "
      ],
      "id": "5443caac"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}